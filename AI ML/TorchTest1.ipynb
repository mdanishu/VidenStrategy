{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244   2022-12-30 00:00:00-05:00\n",
      "1245   2023-01-03 00:00:00-05:00\n",
      "1246   2023-01-04 00:00:00-05:00\n",
      "1247   2023-01-05 00:00:00-05:00\n",
      "1248   2023-01-06 00:00:00-05:00\n",
      "1249   2023-01-09 00:00:00-05:00\n",
      "1250   2023-01-10 00:00:00-05:00\n",
      "1251   2023-01-11 00:00:00-05:00\n",
      "1252   2023-01-12 00:00:00-05:00\n",
      "1253   2023-01-13 00:00:00-05:00\n",
      "1254   2023-01-17 00:00:00-05:00\n",
      "1255   2023-01-18 00:00:00-05:00\n",
      "1256   2023-01-19 00:00:00-05:00\n",
      "1257   2023-01-20 00:00:00-05:00\n",
      "1258   2023-01-23 00:00:00-05:00\n",
      "Name: Date, dtype: datetime64[ns, America/New_York]\n",
      "           Price      Volume\n",
      "0      86.543587  23412800.0\n",
      "1      86.468246  33277500.0\n",
      "2      86.948517  26383200.0\n",
      "3      88.577698  29172200.0\n",
      "4      88.445854  31569900.0\n",
      "...          ...         ...\n",
      "1254  240.350006  29831300.0\n",
      "1255  235.809998  30028700.0\n",
      "1256  231.929993  28623000.0\n",
      "1257  240.220001  35323400.0\n",
      "1258  244.270004  15601977.0\n",
      "\n",
      "[1259 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import sys\n",
    "sys.path.insert(1,'c:/Users/h2pro/Programming')\n",
    "import StockPull\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import seaborn as sns\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# %%\n",
    "#load data\n",
    "company = 'MSFT'\n",
    "start = dt.datetime(2012,1,1)\n",
    "end = dt.datetime(2022,1,1)\n",
    "\n",
    "data = StockPull.analysisout(company)\n",
    "df = data\n",
    "\n",
    "# %%\n",
    "#Separate dates for future plotting\n",
    "train_dates = pd.to_datetime(df['Date'])\n",
    "print(train_dates.tail(15)) #Check last few dates. \n",
    "\n",
    "#Variables for training\n",
    "cols = list(df)[1:3]\n",
    "#New dataframe with only training data - 5 columns\n",
    "df_for_training = df[cols].astype(float)\n",
    "# print(train_data)\n",
    "print(df_for_training)\n",
    "\n",
    "# %%\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)\n",
    "\n",
    "# %%\n",
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 60  # Number of past days we want to use to predict the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Reformat input data into a shape: (n_samples x timesteps x n_features)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#In my example, my df_for_training_scaled has a shape (12823, 5)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_past, \u001b[39mlen\u001b[39m(df_for_training_scaled) \u001b[39m-\u001b[39m n_future \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     trainX\u001b[39m.\u001b[39;49mappend(df_for_training_scaled[i \u001b[39m-\u001b[39m n_past:i, \u001b[39m0\u001b[39m:df_for_training\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]])\n\u001b[0;32m      6\u001b[0m     trainY\u001b[39m.\u001b[39mappend(df_for_training_scaled[i \u001b[39m+\u001b[39m n_future \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:i \u001b[39m+\u001b[39m n_future, \u001b[39m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m trainX, trainY \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(trainX), np\u001b[39m.\u001b[39marray(trainY)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))\n",
    "\n",
    "# %%\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True, return_sequences=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers=2, batch_first=True, return_sequences=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RNNBase.__init__() got an unexpected keyword argument 'return_sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m hidden_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m      3\u001b[0m output_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m LSTMModel(input_size, hidden_size, output_size)\n\u001b[0;32m      7\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m      8\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mLSTMModel.__init__\u001b[1;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_size, hidden_size, output_size):\n\u001b[0;32m     16\u001b[0m     \u001b[39msuper\u001b[39m(LSTMModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLSTM(input_size, hidden_size, num_layers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLSTM(hidden_size, hidden_size, num_layers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_sequences\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\h2pro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:678\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 678\u001b[0m     \u001b[39msuper\u001b[39m(LSTM, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLSTM\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: RNNBase.__init__() got an unexpected keyword argument 'return_sequences'"
     ]
    }
   ],
   "source": [
    "input_size = trainX.shape[2]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 25\n",
    "batch_size = 16\n",
    "validation_split = 0.1\n",
    "\n",
    "trainX = torch.tensor(trainX, dtype=torch.float32)\n",
    "trainY = torch.tensor(trainY, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(trainX, trainY)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(trainX, trainY)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      5\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "\n",
    "# Validation\n",
    "for i, (inputs, labels) in enumerate(valid_loader):\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "\n",
    "    valid_loss += loss.item()\n",
    "\n",
    "print('Epoch {}/{}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, num_epochs, train_loss/len(train_loader), valid_loss/len(valid_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01c730f715953cdae05f8e4ac25ca6d1b5ea3ce06f18038eaf15eb4c2f30d669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
